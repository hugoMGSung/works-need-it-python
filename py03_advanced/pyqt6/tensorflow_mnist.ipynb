{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.8961 - loss: 0.3382\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9844 - loss: 0.0515\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9893 - loss: 0.0327\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9915 - loss: 0.0239\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9943 - loss: 0.0184\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=mnist_tensorflow_model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# 모델 저장\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmnist_tensorflow_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\works-need-it-python\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\works-need-it-python\\venv\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:114\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, zipped, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39msave_model_to_hdf5(\n\u001b[0;32m    112\u001b[0m         model, filepath, overwrite, include_optimizer\n\u001b[0;32m    113\u001b[0m     )\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filepath extension for saving. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease add either a `.keras` extension for the native Keras \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat (recommended) or a `.h5` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `model.export(filepath)` if you want to export a SavedModel \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor use with TFLite/TFServing/etc. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=mnist_tensorflow_model."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# MNIST 데이터셋 로드\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 데이터 전처리\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# 모델 정의\n",
    "model = models.Sequential([\n",
    "    layers.Reshape((28, 28, 1), input_shape=(28, 28)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(x_train, y_train, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# 모델 저장\n",
    "model.save('mnist_tensorflow_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1024) to match target batch_size (64).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     39\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m---> 40\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     42\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\works-need-it-python\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\works-need-it-python\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\works-need-it-python\\venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\01_Programming\\100_HugoBank\\Mine\\works-need-it-python\\venv\\lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (1024) to match target batch_size (64)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# CNN 모델 정의\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(7*7*64, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(-1, 7*7*64)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 데이터 로드\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(1):\n",
    "    for inputs, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 학습된 모델 저장\n",
    "torch.save(model.state_dict(), 'mnist_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current cuda device is cpu\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')\n",
    "\n",
    "print('current cuda device is', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "learning_rate = 0.0001\n",
    "epoch_num = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of traing data: 60000\n",
      "number of test data: 10000\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.MNIST(root = './data',\n",
    "                            train = True,\n",
    "                            download= True,\n",
    "                            transform = transforms.ToTensor())\n",
    "test_data = datasets.MNIST(root = './data',\n",
    "                           train = False,\n",
    "                           transform = transforms.ToTensor())\n",
    "\n",
    "print('number of traing data:',len(train_data))\n",
    "print('number of test data:', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = train_data[0]\n",
    "\n",
    "plt.imshow(image.squeeze().numpy(), cmap ='gray')\n",
    "plt.title('label: %s' % label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset = train_data,\n",
    "                                            batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_data,\n",
    "                                          batch_size = batch_size, shuffle = True)\n",
    "\n",
    "first_batch = train_loader.__iter__().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32,3,1,padding='same')\n",
    "        self.conv2 = nn.Conv2d(32,64,3,1,padding='same')\n",
    "        self.dropout = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(3136,1000) # 7*7*64=3436\n",
    "        self.fc2 = nn.Linear(1000,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterison = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train step: 1000\t loss: 0.172\n",
      "train step: 2000\t loss: 0.130\n",
      "train step: 3000\t loss: 0.069\n",
      "train step: 4000\t loss: 0.047\n",
      "train step: 5000\t loss: 0.023\n",
      "train step: 6000\t loss: 0.006\n",
      "train step: 7000\t loss: 0.004\n",
      "train step: 8000\t loss: 0.024\n",
      "train step: 9000\t loss: 0.005\n",
      "train step: 10000\t loss: 0.008\n",
      "train step: 11000\t loss: 0.017\n",
      "train step: 12000\t loss: 0.014\n",
      "train step: 13000\t loss: 0.001\n",
      "train step: 14000\t loss: 0.008\n",
      "train step: 15000\t loss: 0.002\n",
      "train step: 16000\t loss: 0.001\n",
      "train step: 17000\t loss: 0.004\n",
      "train step: 18000\t loss: 0.002\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "i = 1\n",
    "for epoch in range(epoch_num):\n",
    "    for data, target in train_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterison(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 1000 == 0:\n",
    "            print('train step: {}\\t loss: {:.3f}'.format(i, loss.item()))\n",
    "            \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPU에서 11분 소요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './mnist_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
